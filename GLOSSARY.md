# Glossary

Definitions for common terms used across the repository.

- **Checkpoint**: A saved model state from training used for inference or fine-tuning.
- **Context window**: The maximum amount of text a model can consider in one prompt.
- **Embedding**: A vector representation of text used for similarity search or retrieval.
- **Fine-tuning**: Training a base model on task-specific data to specialize behavior.
- **Inference**: Running a model to generate outputs from inputs.
- **Latency**: Time it takes for a model to return a response.
- **LLM**: Large language model trained to predict text.
- **Prompt**: Input text that guides the model's output.
- **Quantization**: Reducing model precision to speed up inference or reduce memory usage.
- **RAG**: Retrieval-augmented generation; combining search with generation.
- **Temperature**: Sampling parameter controlling randomness of outputs.
- **Throughput**: Number of tokens generated per unit of time.
- **Token**: A unit of text a model processes (often a word piece).
- **Top-p**: Nucleus sampling parameter that limits output to a probability mass.
- **Vector database**: Storage optimized for similarity search over embeddings.
